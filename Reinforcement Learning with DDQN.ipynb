{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import random\n",
    "import gym\n",
    "import math\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "import fastai\n",
    "from fastai.layers import AdaptiveConcatPool2d, conv_layer, Flatten\n",
    "from fastai.text.models import MultiHeadAttention, PositionalEncoding\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick your environment\n",
    "\n",
    "#### you have to hardcode the win condidion of the environment\n",
    "### in the training loop, the current win condition is for\n",
    "### cartpole-v0: average of > 195 reward over 100 consecutive episodes\n",
    "### the curent notebook solves cartpole-v0\n",
    "\n",
    "env = gym.make('CartPole-v0').unwrapped\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env.reset()\n",
    "plt.figure()\n",
    "plt.imshow(utils.get_screen(env).cpu().squeeze(0).permute(1, 2, 0).squeeze(-1).numpy(),\n",
    "           interpolation='none')\n",
    "plt.title('Example extracted screen')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### HELPER FUNCTIONS ###\n",
    "\n",
    "### plotting durations of episodes###\n",
    "def plot_durations():\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    plt.title(str(steps_done))\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    if len(durations_t) >= 10:\n",
    "        means = durations_t.unfold(0, 10, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(9), means))\n",
    "        plt.plot(means.numpy())\n",
    "        \n",
    "    display.display(plt.gcf())\n",
    "    \n",
    "    \n",
    "### action selector for the RL agent ###\n",
    "\n",
    "# epsilon-greedy exploration:\n",
    "# pick random action with probability epsilon,\n",
    "# pick best action with probabilty 1 - epsilon\n",
    "def select_action(state, eps):\n",
    "    global steps_done\n",
    "    sample = sample = random.uniform(0,1)\n",
    "    \n",
    "    steps_done += 1\n",
    "    if (sample > eps) and (steps_done > WARMUP):\n",
    "        with torch.no_grad():\n",
    "            # using the policy network to get the best available \n",
    "            # action\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long)\n",
    "\n",
    "\n",
    "### Deep Q Network archi ###\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    \"\"\"DQN - Playing Atari with Reinforcement learning https://arxiv.org/abs/1312.5602\"\"\"\n",
    "    def __init__(self, cnn, cnn_out_sz, n_actions, act_fn = nn.ReLU()) :\n",
    "        super().__init__()\n",
    "        self.cnn = cnn\n",
    "        self.act_fn = act_fn\n",
    "        self.lin = nn.Linear(cnn_out_sz, 256)\n",
    "        self.value = nn.Linear(256, n_actions)\n",
    "        \n",
    "    def forward(self, frames):\n",
    "        x = self.cnn(frames)\n",
    "        x = self.act_fn(x) # (bs, n_frames * frame_emb_sz)\n",
    "        \n",
    "        x = self.lin(x)\n",
    "        x = self.act_fn(x) # (bs, 512)\n",
    "        \n",
    "        # (bs, n_actions)\n",
    "        return self.value(x) # (bs, n_actions)\n",
    "    \n",
    "    \n",
    "class DuelingDQN(nn.Module) :\n",
    "    \"\"\"Dueling QDN Architecture https://arxiv.org/pdf/1511.06581.pdf\"\"\"\n",
    "    def __init__(self, cnn, cnn_out_sz, n_actions, act_fn = nn.ReLU(inplace=True)) :\n",
    "        super().__init__()\n",
    "        self.cnn = cnn\n",
    "        self.act_fn = act_fn\n",
    "        \n",
    "        self.fc_val = nn.Linear(cnn_out_sz, 256)\n",
    "        self.fc_adv = nn.Linear(cnn_out_sz, 256)\n",
    "        self.value = nn.Linear(256, 1)\n",
    "        self.advantage = nn.Linear(256, n_actions)\n",
    "    \n",
    "    def forward(self, frames) :\n",
    "        x = self.act_fn(self.cnn(frames)) # (bs, n_frames * frame_emb_sz)\n",
    "        \n",
    "        val = self.act_fn(self.fc_val(x))\n",
    "        val = self.value(val)\n",
    "        \n",
    "        adv = self.act_fn(self.fc_adv(x))\n",
    "        adv = self.advantage(adv)\n",
    "        advAverage = torch.mean(adv, dim=1, keepdim=True)\n",
    "        \n",
    "        return val + adv - advAverage\n",
    "    \n",
    "    \n",
    "### Training DQN ###\n",
    "\n",
    "# Getting some previously stored batches of (state, action, next_state, reward)\n",
    "# and training the DQN on them for 1 step.\n",
    "\n",
    "# We're using Double Q-learning (https://arxiv.org/pdf/1509.06461.pdf)\n",
    "# therefore policy and target networks are used.\n",
    "\n",
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = utils.Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "    #loss = loss_func(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN layers\n",
    "layers = [conv_layer(4, 16, stride = 2, ks = 4, norm_type = None), \n",
    "          conv_layer(16, 32, stride = 2, ks = 4, norm_type = None), \n",
    "          conv_layer(32, 64, stride = 2, ks = 4, norm_type = None),\n",
    "          conv_layer(64, 128, stride = 2, ks = 4, norm_type = None),\n",
    "          AdaptiveConcatPool2d(1), \n",
    "          Flatten()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get screen size so that we can initialize layers correctly based on shape\n",
    "# returned from AI gym. Typical dimensions at this point are close to 3x40x90\n",
    "# which is the result of a clamped and down-scaled render buffer in get_screen()\n",
    "init_screen = utils.get_screen(env)\n",
    "_, _, screen_height, screen_width = init_screen.shape\n",
    "\n",
    "# Get number of actions from gym action space\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "policy_net = DuelingDQN(nn.Sequential(*layers), 256, n_actions).to(device)\n",
    "target_net = DuelingDQN(nn.Sequential(*layers), 256, n_actions).to(device)\n",
    "\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "#optimizer = optim.Adam(policy_net.parameters(), lr = 0.0000625)\n",
    "optimizer = optim.RMSprop(policy_net.parameters(), lr = 0.0001)\n",
    "# to store (state, action, next_state, reward) experiences\n",
    "memory = utils.ReplayMemory(50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probability of picking actions at random\n",
    "EPS_START = 1.0\n",
    "EPS_END = 0.03\n",
    "\n",
    "# episode frequency at wich target network is updated\n",
    "TARGET_UPDATE = 60\n",
    "\n",
    "# number of steps before nn is trained\n",
    "WARMUP = 126\n",
    "\n",
    "# total number of episodes\n",
    "num_episodes = 2000\n",
    "\n",
    "eps = EPS_START\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "steps_done = 0\n",
    "rewards = [0]\n",
    "episode_durations = []\n",
    "for i_episode in tqdm_notebook(range(num_episodes)):\n",
    "    # Initialize the environment and state\n",
    "    env.reset()\n",
    "    \n",
    "    acc_reward = 0\n",
    "    \n",
    "    current_screen = utils.get_screen(env)\n",
    "    screen_history = [current_screen]*4\n",
    "    state = torch.cat(screen_history, 1)\n",
    "    \n",
    "    for t in count():\n",
    "        # Select and perform an action\n",
    "        action = select_action(state, eps)\n",
    "        _, reward, done, _ = env.step(action.item())\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        \n",
    "        # Observe new screen\n",
    "        current_screen = utils.get_screen(env)\n",
    "        screen_history = [current_screen] + screen_history[:-1]\n",
    "        if not done:\n",
    "            next_state = torch.cat(screen_history, 1)\n",
    "        else:\n",
    "            next_state = None\n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "        env.render()\n",
    "        acc_reward += reward\n",
    "\n",
    "        # Perform one step of the optimization on the policy network\n",
    "        if t % 4 == 0: \n",
    "            optimize_model()\n",
    "            \n",
    "        if done:\n",
    "            rewards.append(acc_reward.cpu().numpy()[0])\n",
    "            episode_durations.append(t + 1)\n",
    "            break\n",
    "            \n",
    "    # CARTPOLE-V0 WIN CONDITION\n",
    "    if np.mean(rewards[-100:]) >= 195:\n",
    "        print(\"VICTORY\")\n",
    "        break\n",
    "    \n",
    "    # reducing the probability of picking a random action\n",
    "    if steps_done > WARMUP:\n",
    "        eps = max(EPS_END, eps * 0.95)\n",
    "        \n",
    "    # resetting the probability of picking a random action    \n",
    "    if i_episode == (num_episodes // 2):\n",
    "        print(\"Resetting epsilon\")\n",
    "        eps = EPS_START / 3\n",
    "        EPS_END /= 3\n",
    "    \n",
    "    if i_episode % TARGET_UPDATE == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "        print(\"Updating weights\")\n",
    "        print(f\"Average reward since last update: {round(np.mean(rewards),3)}\")\n",
    "        print(f\"Percentage of random actions: {round(eps,3)}\")\n",
    "        rewards = [0]\n",
    "        \n",
    "plot_durations()\n",
    "print('Complete')\n",
    "env.render()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
